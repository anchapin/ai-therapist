name: Voice Features Test Suite

on:
  push:
    branches: [ main, speech_mode ]
  pull_request:
    branches: [ main, speech_mode ]

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: --name ollama

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          portaudio19-dev \
          python3-pyaudio \
          ffmpeg \
          libsndfile1

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-mock
        pip install coverage codecov

    - name: Wait for Ollama service
      run: |
        echo "Waiting for Ollama to start..."
        timeout 60 bash -c 'until curl -s http://localhost:11434/api/tags; do sleep 1; done'
        echo "Ollama is ready!"

    - name: Pull Ollama models
      run: |
        ollama pull llama3.2:latest
        ollama pull nomic-embed-text:latest

    - name: Run Unit Tests
      run: |
        python -m pytest tests/unit/ -v --tb=short --cov=voice --cov-report=xml --cov-report=term-missing

    - name: Run Integration Tests
      run: |
        python -m pytest tests/integration/ -v --tb=medium --cov=voice --cov-report=xml --cov-report=term-missing --cov-append

    - name: Run Security Tests
      run: |
        python -m pytest tests/security/ -v --tb=long --cov=voice --cov-report=xml --cov-report=term-missing --cov-append

    - name: Run Performance Tests
      run: |
        python -m pytest tests/performance/ -v --tb=short --cov=voice --cov-report=xml --cov-report=term-missing --cov-append

    - name: Generate Comprehensive Test Report
      run: |
        python tests/test_runner.py

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: voice-features
        name: codecov-voice-features
        fail_ci_if_error: false

    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          test_report.json
          coverage.xml
          htmlcov/

    - name: Comment test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = './test_report.json';

          if (fs.existsSync(path)) {
            const report = JSON.parse(fs.readFileSync(path, 'utf8'));
            const summary = report.summary;

            const comment = `
            ## üß™ Voice Features Test Results

            **Overall Status**: ${summary.overall_status}
            **Success Rate**: ${Math.round(summary.success_rate * 100)}%
            **Coverage**: ${report.coverage_analysis ? Math.round(report.coverage_analysis.actual_coverage * 100) + '%' : 'N/A'}

            ### Test Categories
            ${Object.entries(report.detailed_results).map(([category, result]) => {
              const icon = result.exit_code === 0 ? '‚úÖ' : '‚ùå';
              return `${icon} ${category}: ${result.description}`;
            }).join('\n')}

            ### Compliance Status
            ${Object.entries(report.compliance_analysis).map(([requirement, status]) => {
              const icon = status.status === 'COMPLETED' ? '‚úÖ' : '‚ùå';
              return `${icon} ${requirement.replace(/_/g, ' ').toUpperCase()}`;
            }).join('\n')}

            ---
            *Full report available in artifacts*
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  security-scan:
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v4

    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r voice/ -f json -o bandit-report.json || true

    - name: Run Safety Check
      run: |
        pip install safety
        safety check --json --output safety-report.json || true

    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      with:
        name: security-scan-results
        path: |
          bandit-report.json
          safety-report.json

  performance-benchmark:
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Run performance benchmarks
      run: |
        python -m pytest tests/performance/ -v --benchmark-only --benchmark-json=benchmark-results.json

    - name: Compare with previous benchmarks
      run: |
        # This would compare with previous benchmarks stored in the repository
        echo "Benchmark comparison logic would go here"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark-results.json